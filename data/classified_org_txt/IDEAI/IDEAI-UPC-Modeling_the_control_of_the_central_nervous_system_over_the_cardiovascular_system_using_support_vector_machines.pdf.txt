















































1 

Modeling the central nervous system control of the cardiovascular system by support 

vector machines 

JosÃ© DÃ­az1, JesÃºs Acosta2, Rafael GonzÃ¡lez1,  

Juan Cota1, Ernesto Sifuentes1, Ã€ngela Nebot3 

1Dept IngenierÃ­a ElÃ©ctrica y ComputaciÃ³n. Instituto de IngenierÃ­a y TecnologÃ­a. 

Universidad AutÃ³noma de Ciudad JuÃ¡rez. 32310 - Ciudad JuÃ¡rez, MÃ©xico. emails: 

david.roman@uacj.mx, rafael.gonzalez@uacj.mx, jcota@uacj.mx, esifuent@uacj.mx. 

2Dept InstrumentaciÃ³n y Control. Universidad PolitÃ©cnica Territorial de FalcÃ³n â€œAlonso 

Gameroâ€ (UPTAG). 4101 - Coro, Venezuela. e-mail: ajesus@uptag.edu.ve.  

3Dept Computer Science. Universitat PolitÃ¨cnica de Catalunya. 08034 - Barcelona, Spain.   

e-mail: angela@cs.upc.edu.  

Corresponding author: 

JosÃ© David DÃ­az R. 

Email: david.roman@uacj.mx 

Phone: +52 (656) 688 4841 

Â© 2017 Elsevier. This manuscript version is made available under the CC-BY-NC-ND 4.0 license 
http://creativecommons.org/licenses/by-nc-nd/4.0/ 



2 
 

Abstract 

The control of the cardiovascular system (CS) has been scarcely modeled due, basically, to 

the complexity of the central nervous system (CNS). The cardiovascular and the central 

nervous systems are closely related one to each other. Some models based on fuzzy inductive 

reasoning, genetic fuzzy systems, neural network and quantitative approaches using 

nonlinear with autoregressive techniques, have been employed  for this purpose, resulting in 

a significant, but still improvable approximation of the expected control response of the CNS. 

In this research, we used support vector machines for predicting the response of a branch of 

the CNS, which controls an important part of the cardiovascular system. In this study, five 

controllers that generate signals that regulate heart rate, myocardial contractility, peripheral 

resistance, venous tone, and coronary resistance, and that react to variations of the carotid 

sinus blood pressure (CSBP), are presented. The predictive models were trained using an 

input signal (i.e., CSBP) and an output signal for each controller; and a set of six input and 

output signals were employed for testing each controller model. Input signals were processed 

using an all-pass filter, and the performance of the predictive models were evaluated using 

the average of the normalized mean square error (MSE) in percentage. The best forecasting 

performance was obtained for the peripheral resistance controller with a MSE= 1.20e-4 %, 

and the worst for the heart rate controller with a MSE= 1.80e-3 %. Support vector machines 

presented a better performance estimating the dynamical behavior of the control of the CNS 

over the cardiovascular system than other modeling systems previously studied.  

 

Key words: Central nervous system, cardiovascular system, modeling, support vector 

machine, fuzzy inductive reasoning. 



3 
 

 

1. Introduction 

 

The mechanisms of regulation of visceral organs in the nervous system has been historically 

a research topic. One of the most important systems of the body is the cardiovascular system 

and, obviously, it is almost fully controlled by the central nervous system. The CNS generates 

regulatory signals which are transmitted by bundles of nerves through the autonomic nervous 

system (sympathetic and parasympathetic) to the heart, blood vessels, kidneys, and other 

body parts; this allows to maintain an appropriate blood flow, following hemodynamic 

changes due mainly to variations of the arterial blood pressure. The CNS controls important 

global functions in the CS such as: cardiac output (modifying the heart rate and myocardial 

contractility), redistribution of blood flow, and rapid control of the arterial pressure to 

mention a few [1].  

 

Most of the control generated by the CNS over the CS is carried out in the vasomotor center 

which manages integral information from: visceral sensors (coming from baroreceptors of 

cardiac cavities and large vessels, and from chemoreceptors), the bulbar respiratory center, 

and other suprasegmental structures of the brain [2]. The reflex function of baroreceptors is 

very important in rapid control of blood pressure, which modulates efferent signals directed 

to the heart and blood vessels. It is well known that baroreflex action produces rapid changes 

in renal sympathetic nervous activity, which plays a very important role in the short term 

control of the blood pressure carrying out a variety of functions like mediating renin 

secretion, tubular reabsorption of water and sodium, and renal intravascular resistance [3, 4]. 



4 
 

 

The hemodynamic behavior of the CS has been widely studied, and there are many 

mathematical and computational models that permit fairly and accurately simulate the 

performance of the hemodynamic variables of this system [5-7]. Moreover, due the high 

complexity of the CNS, modeling the control response over the components of the 

hemodynamic system (heart and blood vessels) can represent an important challenge.  

 

A simplified diagram of the cardiovascular system proposed by ValverdÃº [7] is shown in Fig. 

1. In the diagram, hemodynamic system (HS), controlled by the CNS, makes up the 

cardiovascular system. The branch of the CNS that controls the hemodynamic system is 

composed of five controllers that produce efferent signals leading changes in the peripheral 

resistance, the cardiac output and the coronary circulation. The controllers are: heart rate 

(HR), myocardial contractility (MC), peripheral resistance (PR), venous tone (VT), and 

coronary resistance (CR). The afferent signal that drives these controllers is represented by 

the carotid sinus blood pressure, originated from the arterial carotid sinus baroreceptors [7].  

 



5 
 

 

Fig. 1. Simplified diagram of the cardiovascular system model composed of the CNS control 

and the hemodynamic system [7]. 

 

In the last few decades, important contributions have been produced, both in theory and in 

practice, related to the support vector machines (SVM) approach. This approach has led to 

the development of methodologies useful in the design of efficient algorithms with 

applications focused to practical problems [8-10]. Linear and non-linear SVM regression for 

time series prediction have been widely used in many real word applications such as 

forecasting of financial market, electric utility, weather, traffic, among others [11]; however, 

there is not much literature that outline approaches to predict biomedical signals. In one of 

these studies, Shen et al. [12] developed a predictor model based on a wavelet kernel function 

for a SVM in order to predict multichannel electroencephalogram signals. It has been shown 

that SVMs is a methodology that captures effectively the dynamism of system processes 

which are typically nonlinear, non-stationary and not defined a-priori. 



6 
 

 

The purpose of this study is to predict the output of the five CNS controllers in the 

cardiovascular system model proposed by ValverdÃº [7], employing regression models of 

support vector machines. Previous investigations have employed NARMAX (nonlinear 

autoregressive moving average with external inputs) models [13], neural networks [14], 

fuzzy inductive reasoning (FIR) [13, 15], genetic fuzzy systems (GFS) or hybrid techniques 

such as genetic-FIR algorithm (GA-FIR) [16], and an automatic construction of linguistic 

rules methodology based on FIR (CARFIR) [17], for the same target. Although some of these 

models have reported good results, in the present study the authors intend to explore the 

potentialities of support vector machines in this prediction task, which due to its great 

complexity, represents a major challenge for any nonlinear regression model. The authors 

propose a new predictive model derived from SVMs which combines digital signal 

processing, offering an alternative to build a model that can be as robust and efficient as 

others already mentioned. The performance of the proposed SVM model is compared with 

those obtained in previous studies. 

 

2. The Support Vector Machines Methodology 

 

According to the theory of SVMs, instead of minimizing the empirical risk (to optimize the 

training set performance), which is the target of traditional techniques for pattern recognition, 

SVMs minimizes an upper bound on the expected risk (structural risk). It supplies SVM with 

a great ability to generalize any model, the main goal in statistical learning. 

 



7 
 

There are three important characteristics of the SVMs [18]:  

1) With few training data points, the learning technique generalizes the model; and from this 

training data set, the generalization error limit can be estimated.  

2) There is only one variable acting as a regularizing parameter (associated to the penalty for 

misclassification) [19], which determines a balance between generalization performance and 

resolution  [20].  

3) To obtain the best performance with data not included in the training, the algorithm finds 

a decision surface that maximizes the margin between the classified data in the training.  

 

Support vector machines can be used for both classification and regression tasks. SVMs 

determinate the output as a linear combination of samples in the training data, in which the 

data points with nonzero coefficients are called â€œsupport vectorsâ€.  

 

Given a set of data training composed of ğ‘™ attribute-label pairs (ğ‘¥ğ‘–, ğ‘¦ğ‘–), ğ‘– = 1, â€¦ , ğ‘™; where 

ğ‘¥ğ‘– âˆˆ ğ‘…
ğ‘› and ğ‘¦ğ‘– âˆˆ  {1, âˆ’1}, for classification purposes, the SVM needs the solution of the 

optimization problem posed in Equation (1) under the constraints describes in the Equation 

(2) [21]: 

 

min 
 

Î¦(w,Î¾) =
1

2
ğ‘¤ğ‘‡ğ‘¤ + ğ¶ âˆ‘ ğœ‰ğ‘–

ğ‘™

ğ‘–=1

 1 

subject to  
ğ‘¦ğ‘–(ğ‘¤

ğ‘‡ğœ™(ğ‘¥ğ‘–) + ğ‘) â‰¥ 1 âˆ’ ğœ‰ğ‘– 

ğœ‰ğ‘– â‰¥ 0 
2 



8 
 

 

Here, training vectors ğ‘¥ğ‘– are mapped by the function ğœ™ into a higher dimensional space; in 

this higher dimensional space, SVM finds a linear separating hyperplane with the maximal 

margin. The vector ğ‘¤ determines the generalized optimal separating hyperplane. ğœ‰ğ‘– 

represents a measure of the misclassification errors, and ğ¶ > 0 (a settable parameter) is the 

cost parameter of the error term.  

 

The solution to the optimization problem of Equation (1) under the constraints of Equation 

(2) is equivalent to determine the point at which the gradient of the Lagrangian is zero: 

 

Î¦(w,b,Î±,Î¾,Î²) =
1

2
â€–ğ‘¤â€–2 + ğ¶ âˆ‘ ğœ‰ğ‘– âˆ’ âˆ‘ ğ›¼ğ‘–[ğ‘¦ğ‘–(ğ‘¤

ğ‘‡ğœ™(ğ‘¥ğ‘–) + ğ‘) âˆ’ 1 + ğœ‰ğ‘–]

ğ‘™

ğ‘–=1

âˆ‘ ğ›½ğ‘–ğœ‰ğ‘–

ğ‘™

ğ‘–=1

ğ‘™

ğ‘–=1

  3 

 

where Î±, Î² are the Lagrange multipliers. An easy way to solve Equation (3) is to transform 

the primal problem in a dual problem, by minimizing the Lagrangian with respect to ğ‘¤, b, ğœ‰, 

and maximizing it with respect to Î±, Î²; which is given by, 

 

max
ğ›¼

ğ‘Š(ğ›¼, ğ›½) = max
ğ›¼,ğ›½

(min
ğ‘¤,ğ‘,ğœ‰

Î¦(ğ‘¤, ğ‘, ğ›¼, ğ›½, ğœ‰)) 
 

4 

 

Equation (5) represents the minimum of the Lagrangian Î¦ with respect to ğ‘¤, b and ğœ‰: 

 



9 
 

 ğœ•Î¦

ğœ•ğ‘
= 0  â‡’ âˆ‘ ğ›¼ğ‘–ğ‘¦ğ‘– = 0

ğ‘™

ğ‘–=1

 

ğœ•Î¦

ğœ•ğ‘¤
= 0  â‡’  ğ‘¤ = âˆ‘ ğ›¼ğ‘–ğ‘¦ğ‘–ğœ™(ğ‘¥ğ‘–) = 0

ğ‘™

ğ‘–=1

 

ğœ•Î¦

ğœ•ğœ‰
= 0  â‡’ ğ›¼ğ‘– + ğ›½ğ‘– = ğ¶ 

 

5 

 

Then, the solution of the optimization problem in the Equation (5) becomes: 

 

ğ›¼âˆ— = arg min
ğ›¼

1

2
âˆ‘ âˆ‘ ğ›¼ğ‘–ğ›¼ğ‘—ğ‘¦ğ‘–ğ‘¦ğ‘–ğ¾(ğ‘¥ğ‘– , ğ‘¥ğ‘—)

ğ‘™

ğ‘—=1

âˆ’ âˆ‘ ğ›¼ğ‘˜

ğ‘™

ğ‘˜=1

ğ‘™

ğ‘–=1

 

 

6 

 

ğ¾(ğ‘¥ğ‘–, ğ‘¥ğ‘—) â‰¡ ğœ™(ğ‘¥ğ‘–)
ğ‘‡ğœ™(ğ‘¥ğ‘—) represents the kernel function, which makes a nonlinear mapping 

of the input space into a feature space (usually with a higher dimension).The constraints are: 

 

0 â‰¤ ğ›¼ğ‘– â‰¤ ğ¶   ğ‘– = 1, â€¦ , ğ‘™ 

âˆ‘ ğ›¼ğ‘—ğ‘¦ğ‘— = 0

ğ‘™

ğ‘—=1

 

 

7 

 

Solving Equation (6) with constraints in (7) determines the Lagrange multipliers. Equation 

(8) establishes the classifier for the optimal separating hyperplane in the feature space.   

 



10 
 

ğ‘“(ğ‘¥) = ğ‘ ğ‘”ğ‘› (âˆ‘ ğ›¼ğ‘–ğ‘¦ğ‘–ğ¾(ğ‘¥ğ‘–, ğ‘¥)

ğ‘™

ğ‘–=1

+ ğ‘) 
 

8 

 

The role of the kernel then is to change the representation of the data into another feature 

space. Popular kernels used for pattern recognition are the followings: 

 

ğ¾(ğ‘¥, ğ‘¦) = (1 + ğ‘¥ âˆ™ ğ‘¦)ğ‘  9 

ğ¾(ğ‘¥, ğ‘¦) = ğ‘’ğ‘¥ğ‘ (âˆ’
â€–ğ‘¥ âˆ’ ğ‘¦â€–2

2ğœ2
) 

 
10 

ğ¾(ğ‘¥, ğ‘¦) = tanh(ğ‘˜ğ‘¥ âˆ™ ğ‘¦ âˆ’ ğ›¿) for some ğ‘˜ and ğ›¿  11 

 

Equation (9) represents a polynomial kernel. It provides a classifier, which is a â€œpâ€ degree 

polynomial over the data. Equation (10) gives a classifier based on Gaussian radial basis 

functions, and Equation (11) provides a kernel that represents a kind of special neural 

network of a hide layer with sigmoid activation functions. Most of the kernel functions are 

presented and detailed in [18, 22, 23]. 

 

For regression problems, SVM includes an alternative loss function [24], which must be 

modified to introduce a measure of distance. Four examples of loss function for 

implementing SVM regression models are presented in [23, 24].  

 



11 
 

For regression tasks, given a training set of l data points, (ğ‘¥ğ‘–, ğ‘§ğ‘–), ğ‘– = 1, â€¦ , ğ‘™; where ğ‘¥ğ‘–  âˆˆ  ğ‘…
ğ‘› 

is an input and ğ‘§ğ‘–  âˆˆ  ğ‘…
1 is the corresponding output, the optimization problem that needs to 

be solved [18] is: 

 

ğ‘šğ‘–ğ‘›
ğ‘¤,ğ‘,ğœ‰,ğœ‰âˆ—

 
 1

2
ğ‘¤ğ‘‡ğ‘¤ + ğ¶ âˆ‘ ğœ‰ğ‘–

ğ‘™

ğ‘–=1

+ ğ¶ âˆ‘ ğœ‰ğ‘–
âˆ—

ğ‘™

ğ‘–=1

 12 

subject to 

 (ğ‘¤ğ‘‡ğœ™(ğ‘¥ğ‘–) + ğ‘ âˆ’ ğ‘§ğ‘–) â‰¤ ğœ– âˆ’ ğœ‰ğ‘– 

(ğ‘§ğ‘– âˆ’ ğ‘¤
ğ‘‡ğœ™(ğ‘¥ğ‘–) âˆ’ ğ‘) â‰¤ ğœ– âˆ’ ğœ‰ğ‘–

âˆ— 

ğœ‰ğ‘– , ğœ‰ğ‘–
âˆ— â‰¥ 0, ğ‘– = 1, â€¦ , ğ‘™ 

13 

 

where C is a pre-specified value (cost parameter of the error term, like in classifier SVMs), 

and ğœ‰ğ‘–, ğœ‰ğ‘–
âˆ— are looseness variables that set upper and lower limits on the outputs of the system. 

 

When using a ğœ–-insensitive loss function [23], the dual problem is expressed by: 

 

ğ‘šğ‘–ğ‘›
ğ›¼,ğ›¼âˆ—

 
 1

2
(ğ›¼ âˆ’ ğ›¼âˆ—)ğ‘‡ğ‘„(ğ›¼ âˆ’ ğ›¼âˆ—) + ğ¶ âˆ‘(ğ›¼ğ‘– âˆ’ ğ›¼ğ‘–

âˆ—)

ğ‘™

ğ‘–=1

+ âˆ‘ ğ‘§ğ‘–(ğ›¼ğ‘– âˆ’ ğ›¼ğ‘–
âˆ—)

ğ‘™

ğ‘–=1

 14 

subject to 
 

âˆ‘(ğ›¼ğ‘– âˆ’ ğ›¼ğ‘–
âˆ—)

ğ‘™

ğ‘–=1

= 0, 0 â‰¤ ğ›¼ğ‘– ,        ğ›¼ğ‘–
âˆ— â‰¤ ğ¶, ğ‘– = 1, â€¦ , ğ‘™  15 

 

where ğ‘„ğ‘–ğ‘— = ğ¾(ğ‘¥ğ‘– , ğ‘¥ğ‘—) â‰¡ ğœ™(ğ‘¥ğ‘–)
ğ‘‡ğœ™(ğ‘¥ğ‘—).  

 



12 
 

Lagrange multipliers, Î±, Î±*, are determined by solving Equation (14) with the constraints of 

the Equation (15). The regression function is given by:  

 

ğ‘“(ğ‘¥) = âˆ‘(ğ›¼ï¿½Ì…ï¿½ âˆ’ ğ›¼ğ‘–
âˆ—Ì…Ì… Ì…)ğ¾

ğ‘™

ğ‘–=1

(ğ‘¥ğ‘–, ğ‘¥) + ï¿½Ì…ï¿½ 
 

16 

 

where 

 

ğ‘¤ğ‘‡ğœ™(ğ‘¥ğ‘–) = âˆ‘(ğ›¼ğ‘–, ğ›¼ğ‘–
âˆ—)

ğ‘™

ğ‘–=1

ğ¾(ğ‘¥ğ‘–, ğ‘¥) 

ï¿½Ì…ï¿½ = âˆ’
1

2
âˆ‘(ğ›¼ğ‘–, ğ›¼ğ‘–

âˆ—)

ğ‘™

ğ‘–=1

(ğ¾(ğ‘¥ğ‘– , ğ‘¥ğ‘Ÿ) + ğ¾(ğ‘¥ğ‘–, ğ‘¥ğ‘ )) 

 

17 

 

The Kernel function could contain a bias term b. In that case, the regression function is given 

by: 

 

ğ‘“(ğ‘¥) = âˆ‘(ğ›¼ï¿½Ì…ï¿½ âˆ’ ğ›¼ğ‘–
âˆ—Ì…Ì… Ì…)ğ¾

ğ‘™

ğ‘–=1

(ğ‘¥ğ‘–, ğ‘¥) 
 

18 

 

The optimization criteria for the other loss functions are similarly to those ones obtained in 

[24]. The ğœ–-insensitive loss function is attractive because the solution can contain few support 

vectors, instead, in the quadratic and Huber cost functions, all data points are support vectors.  

 



13 
 

In this investigation, support vector machine regression models are implemented in Matlab 

[25] using the LIBSVM tool developed by the Taiwan University [26].  

 

3. Cardiovascular Control of the Central Nervous System  

 

The specific data used in the present study corresponds to the same signal sets used by 

VallverdÃº in [7], where a generic model of the CS was identified and validated. Cardiac 

catheterization of patients was employed to obtain physiological data in order to validate the 

model. This led to the simplified cardiovascular system model shown in the diagram of Fig. 

1. The branch of the CNS that controls the hemodynamic system is composed of five 

controllers that produce efferent signals to lead changes in peripheral resistance, cardiac 

output and coronary circulation; these are: heart rate, myocardial contractility, peripheral 

resistance, venous tone, and coronary resistance. The afferent signal for driven these 

controllers is represented by the carotid sinus blood pressure, originated from the arterial 

carotid sinus baroreceptors [7]. Sensibility analysis of the ValverdÃºâ€™s study showed that the 

parameters of the baroreceptors having the most influence on the output variables of the 

model were those of the carotid sinus baroreceptors, so the influence of the baroreceptors of 

the aortic arch is neglected. It is important to notice that, in this CS model, the regulatory 

mechanisms of the renal function, in response to changes of the renal sympathetic nerve 

activity (RSNA) mediated by baroreflex action, is not assessed due to the fact that  RSNA 

data is not available.  

 



14 
 

The signals were obtained simulating a model of differential equations of the central nervous 

system [7]. The model of differential equations was tuned in order to represent patients with 

different percentages of coronary arterial obstruction (between 30 % and 70 %) by making 

agree the four physiologic variables of the model (heart rate, aortic pressure, right auricular 

pressure, and coronary blood flow) with measurements taken from the patients. It was used 

7279 data samples of the input and output signal of each CNS controller to identify each 

model (the training data set), from simulations of the differential equation model with a 

sampling period of Ts=0.12 s [7]. Fig. 2 and Fig. 3 show the training signals of the input 

variable (CSBP) and the output variables of the controllers.  

 

 



15 
 

Fig. 2. From top to bottom, the carotid sinus blood pressure (input), and the output signals of 

the controllers of venous tone and peripheral resistance, used for training the SVM models.  

 

 

Fig. 3. From top to bottom, the output signals of the controllers of myocardial contractility, 

heart rate, and coronary resistance, used for training the SVM models. 

 

A set of test data with six signal segments (not employed in training) were used to evaluate 

the models of each controller. Each test data segments has a size of about 600 samples. These 

contain signals with specific morphologies, product of performing one standardized Valsalva 

maneuver and five different exhibitions of this maneuver with different duration and relative 



16 
 

intensity. A detailed explanation of the test data sets can be found in [7, 13, 15]. Fig.4 shows 

the output signals (test data set) of the heart rate controller.  

 

 

Fig. 4. Heart rate controller test data set.  

 

3.1 Training Process 

 

An all-pass filter (two pole/cero pairs) was used to create a signal with time delay (phase 

compensation), without affecting its magnitude response, which could be used as a second 

input signal to the models. Preliminary attempts using only one input signal (and different 



17 
 

processed version of that) for training the SVM model presented a very poor performance in 

the training process. For this reason, a second input signal for the model had to be chosen. 

 

Primarily, start and end frequencies of the pole/cero pair spacing of the all-pass filter were 

determined. It was used a coarse (0.1 Ï€ step) grained searching in the frequency plane  

[0 â€“ Ï€], followed by a fine (0.01 Ï€ step) grained searching around the frequency pair with the 

best performance found previously. Pole magnitude was set to 1
âˆš2

â„ . It was used a Gaussian 

radial basis function (RBF) kernel, described in Equation (10), for many advantages over 

other kernel functions as recommended by Hsu et al. [27]. Preliminary tests were made with 

different kernels, and RBF approach showed the best performance on the prediction task over 

polynomial and sigmoid functions, and it was also faster during the training than others. In 

this case, we used C=10 and Ïƒ=10 for the SVM model, and the performance of each iteration 

employing the whole training data for each controller was analyzed.  

 

The model performance of each iteration was determined by computing the normalized mean 

square error (MSE), in percentage, between the predicted output of the model (ï¿½Ì‚ï¿½(ğ‘›)) and the 

expected system output (ğ‘¦(ğ‘›)). The error was computed by the Equation (19): 

 

ğ‘€ğ‘†ğ¸ =
ğ¸ [(ğ‘¦(ğ‘›) âˆ’ ï¿½Ì‚ï¿½(ğ‘›))

2
]

ğ‘¦ğ‘£ğ‘ğ‘Ÿ
âˆ™ 100% 

 
19 

 

where ğ‘¦ğ‘£ğ‘ğ‘Ÿ is the variance of ğ‘¦(ğ‘›), given by:  



18 
 

 

ğ‘¦ğ‘£ğ‘ğ‘Ÿ = ğ¸[ğ‘¦(ğ‘›)
2 ] âˆ’ {ğ¸[ğ‘¦ğ‘›]}

2  20 

 

A cross validation process was employed in order to adjust the parameters of the SVM 

models. To do this, training dataset of each controller was segmented in subsets of 4279 data 

samples, in which 3679 samples (about 50.5 % of the total training dataset) were used for 

training the model, and the other 600 samples were used for testing proposes. Only 

continuous segments of the data were considered for training and testing (to avoid 

discontinuousness in the signal processing). In total, six subsets of data were formed sliding 

the 4279 samples window from the beginning to the end of the data with 3679 overlapped 

samples in each case. Twelve iterations of cross validation were performed; in six cases the 

test data was located at the end of the data subsets (testing data subsets â€œaâ€), and in the others, 

it was at the beginning of the data subsets (testing data subsets â€œbâ€). This process is shown 

in Fig. 5. In this figure the subplot a) contains 4279 data samples of the subset 1, where the 

training data represent the first 3679 samples, and the remaining 600 ones were used to test 

the model (subset â€œaâ€ in this case); subplot b) represents the data window of 4279 samples 

which is moved to the right to incorporate 600 new samples (overlapping 3679 samples of 

the subset 1) forming the subset 2; subplot c) presents the last data window by sliding step 

by step to the end of the training data set in order to conform a total of 6 data subsets; and 

subplot d) represents the version â€œbâ€ of the data subset 1; the first 600 samples represent the 

data for testing and the following 3679 samples were used for training the model. 

 



19 
 

 

Fig. 5. Data segmentation for cross validation process. 

 

The MSE average of the 12 iterations in the cross validation process was computed to find 

the best parameters C and Ïƒ of the SVM regression model by using the grid-search procedure 

as suggested in [27]. 

 

3.2 Validation Process 

 



20 
 

Only the parameters that produce models with the best performance obtained in the cross 

validation process were employed in the evaluation phase. Once all parameters of signal 

processing and SVM models were found, a final model was built including the whole training 

data set (7279 samples) for each controller. Set parameters of the all-pass filter (start and stop 

frequency) and the SVM models (C and Ïƒ), for each controller are shown in Table 1. 

 

Table 1. Parameters used in the all past filter and SVM for each controller 

  HR PR MC VT CR 

All-pass filter 

Start - stop  

frequency [Ï€ rad] 

.01 - .11 .01 - .13 .01 - .13 .01 - .11 .01 - .13 

SVM 

C 10 10 10 5 5 

Ïƒ 10 53 60 58 60 

 

Support vector machine regression models of each CNS controller were then evaluated using 

the testing data set. The MSE obtained when evaluating the models with the six test data sets 

and their average were reported in the results section. 

 

3.3 Results  

 

Table 2 shows the MSE of the predictions obtained by the five controller SVM models. 

Looking closer to Table 2 it can be seen that the largest average error was 1.8e-3 % (for the 

HR controller). 

 



21 
 

Table 2. Prediction MSE for the validation data sets of each controller model using SVM  

 HR (%) PR (%) MC (%) VT (%) CR (%) 

Training data 1.7e-3 1.14e-4 6.38e-4 7.05e-4 1.36e-4 

Data set 1 1.7e-3 1.10e-4 5.34e-4 6.06e-4 1.05e-4 

Data set 2 1.5e-3 1.10e-4 4.61e-4 5.81e-4 0.99e-4 

Data set 3 1.5e-3 0.89e-4 4.97e-4 4.55e-4 1.08e-4 

Data set 4 1.6e-3 1.03e-4 5.32e-4 5.98e-4 1.39e-4 

Data set 5 2.0e-3 1.31e-4 7.44e-4 7.55e-4 1.57e-4 

Data set 6 2.8e-3 1.83e-4 9.42e-4 1.10e-3 1.80e-4 

Average Error 1.8e-3 1.20e-4 6.18e-4 6.81e-4 1.36e-4 

 

Fig. 6 shows the results of the prediction for the test data set 3 of the peripheral resistance 

controller. It presents the best prediction results obtained, with a MSE= 0.89e-4 %. In the 

upper plot of this figure, the expected and predicted output signals are presented in the same 

frame. Differences between these signals cannot be appreciated, demonstrating the excellent 

performance of the predictions of our models. The prediction errors (real minus predicted 

values) is shown in the lower plot of this figure.  

 

The worst case is shown in Fig. 7, which corresponds to the prediction of the test data set 6 

of the heart rate controller. Even in this case, differences between expected and predicted 

signals are not distinguishable, which means a great precision of our model. 

 

It is important to notice that the processing time to execute 416 iterations to find the 

frequencies of the all pass filter is about 4992 seconds (in the worst case), and to find the 



22 
 

parameters C and Ïƒ of the SVM model is 44256 seconds. Therefore, in order to perform all 

the iterations of the cross validation process the total time needed is 5532 seconds. The final 

training of the SVM model, with the complete training data of each controller, takes 12 

seconds at the most. All Matlab simulations were made with a Core 2 Duo computer with a 

2.1 GHz processor. 

 

 

Fig.6. Peripheral resistance controller: Prediction of the test data set 3. 

 

 

 



23 
 

 

Fig 7. Heart rate controller: Prediction of the test data set 6. 

 

3.4 Discussion 

 

In this section, the performance of the SVM regression models of the present work is 

compared with those obtained in previous studies.  

 

Table 3 contains the prediction results reported for the same problem when using NARMAX, 

TDNN RNN, FIR, GFS (GA-FIR) and CARFIR models. The table specifies the MSE average 

of the test data set prediction, for each controller. Row six of this table presents the best 

performance obtained by Nebot et al. [15]. In that research the FIR methodology was used to 



24 
 

predict the test data sets (which are not employed in the training process), of the five 

controllers. Nebot and coworkers used a database that includes signals of five different 

patients. The results presented in Table 3 correspond to the model build for the patient #4, 

which had the smallest MSE average, when compared with the other patientsâ€™ results. In the 

current research, the obtained average error is slightly higher when compared with those 

obtained by FIR models for patient #4. However, in average, the error obtain using SVMs is 

lower than the one obtained for the rest of the patients in the Nebot study. 

 

Table 3. MSE obtained when predicting the validation data set by using different methods 

 HR (%) PR (%) MC (%) VT (%) CR (%) 

NARMAX [13] 9.8 14.89 17.21 16.89 31.69 

FIR [13] 1.37 1.49 1.41 1.47 0.09 

TDNN [14] 15.35 33.76 34.02 34.04 55.69 

RNN [14] 18.31 31.16 35.16 34.77 57.12 

FIR* [15] 7.3e-5 7.0e-4 7.6e-6 7.9e-4 3.0e-4 

GFS (GA-FIR) [16] 0.10 0.15 0.30 0.28 9.47e-30 

CARFIR [17] 11.02 9.97 5.00 5.01 2.64 

*Patient 4 

 

The worst performance reported for our SVM model of the HR controller (MSE average of 

1.8e-3 %), corresponds to the test data set #6 with a MSE= 2.8e-3 % (see table 1, sixth 

row/first column). It is considerably lower than the one reported by Nebot [15] for the HR 



25 
 

controller model of patient #4 (MSE average of 7.3e-5 %), predicting the same test data set 

with a MSE= 32.1e-3 %. This means that SVM models fit very well to all test data sets for 

each controller. Therefore, it shows a better performance than other predictive models. 

 

Table 4. Comparison of the best results in the prediction of the output controllers 

 HR (%) PR (%) MC (%) VT (%) CR (%) 

SVM (this study) 1.8e-3 1.20e-4 6.18e-4 6.81e-4 1.36e-4 

FIR [15] 7.3e-5 7.0e-4 7.6e-6 7.9e-4 3.0e-4 

GFS (GA-FIR) [16] 0.10 0.15 0.30 0.28 9.47e-30 

 

In the present study, the MSE average for the MC controller was higher than the one reported 

by Nebot [15], nevertheless it is probable that the error in some of their validation sets had 

been worse than the ones obtained by the SVM models for this controller. Moreover, in [15], 

there are not enough details of the results for each controller in order to make a precise 

comparison. 

 

It should be also noted, that while the prediction for the CR controller (see Table 4) is better 

than the one obtained by Nebot [15], it does not exceed the obtained by the GFS technique 

[16]. However, it should be taken into account the computational cost required to use that 

hybrid technique. The total execution time for tuning the parameters and training the SVM 

model of each controller was less than 14 hours. Acosta in [16] reported an average time of 

more than 357 hour for finding the best parameters of the GFS model, employing a computer 



26 
 

with a 0.6 GHz processor. Although, the computer processor used in our investigation is 3.5 

times faster than Acostaâ€™s processor, the required lapse to build the SVM model is close to 

25.5 times shorter; it means a computational savings near 7.3 times. 

 

4. Conclusions 

 

In this paper support vector machines have been used for modeling a portion of the human 

central nervous system control, which is in charge of controlling the hemodynamic behavior 

of the cardiovascular system. The controllers of heart rate, myocardial contractility, 

peripheral resistance, venous tone and coronary resistance have been modeled in order to 

predict their responses driven by the same input variable, the carotid sinus blood pressure, 

under different systemsâ€™ behaviors. 

 

Methodological considerations were presented for tuning the parameters for processing the 

input signals and the fitting of the SVM variables. The choice of a second input signal as a 

group delayed version created from the original signal processing, was a key factor to obtain 

an excellent performance of the SVM prediction models. 

 

The low computational cost of implementing the training method for these SVM models 

indicates that it is a very simple and flexible strategy, and it compares very favorably with 

other optimization techniques implemented in other models with the same purpose. 

 



27 
 

The purpose of the present study was to predict CNS control responses to transient changes 

in blood pressure (as caused by Valsalva maneuvers), by means of SVM regression models. 

Changes in blood pressure cause changes in the afferent baroreceptor signal (which 

represented the input signal of our model). To close the control loop, the outputs of our model 

should activate the effector elements of the hemodynamic system (heart and blood vessels), 

that were modeled in the VallverdÃºâ€™s study. However, the model of the hemodynamic system 

was not the focus of the research effort of this paper. 

 

It has been shown that support vector machines represent an efficient and powerful 

methodology, which is proficient in modeling the dynamical behavior of the control of the 

CNS over the cardiovascular system. 

 

 

References 

 

[1] A.C. Guyton, J.E. Hall, Tratado de FisiologÃ­a MÃ©dica, 10th ed., Mc-Graw Hill 

Interamiericana, Mexico, 2006. 

[2] J.H. Coote, Landmarks in understanding the central nervous control of the cardiovascular 

system, Experimental Physiology, 92 (2007) 3-18. 

[3] U.C. Kopp, Neural control of renal function, Morgan & Claypool Life Sciences, San 

Rafael (CA), 2011. 

[4] C. Julien, Baroreflex control of sympathetic nerve activity and blood pressure variability, 

Clinical and experimental pharmacology & physiology, 35 (2008) 512-515. 



28 
 

[5] J.E.W. Beneken, V.C. Rideout, The Use of Multiple Models in Cardiovascular System 

Studies: Transport and Perturbation Methods, IEEE Transactions on Biomedical Engineering 

15 (1968) 281-289. 

[6] Sagawa K., Maughan L., Suga H., S. K., Cardiac Contraction and the Pressure-Volume 

Relationship, New York, 1988. 

[7] M. VallverdÃº, Modelado y simulaciÃ³n del sistema de control cardiovascular en pacientes 

con lesiones coronarias, Universitat PolitÃ¨cnica de Catalunya, Barcelona, 1993. 

[8] Y. Ma, G. Guo, Support vector machines applications, Springer, New York, 2014. 

[9] D. Naiyang, T. Yingjie, Z. Chunhua, Support Vector Machines: Optimization Based 

Theory, Algorithms, and Extensions, Chapman \& Hall/CRC2012. 

[10] R.G. Brereton, G.R. Lloyd, Support Vector Machines for classification and regression, 

Analyst, 135 (2010) 230-267. 

[11] N.I. Sapankevych, R. Sankar, Time Series Prediction Using Support Vector Machines: 

A Survey, IEEE Computational Intelligence Magazine, 4 (2009) 24-38. 

[12] M. Shen, L. Lin, J. Chen, C.Q. Chang, A Prediction Approach for Multichannel EEG 

Signals Modeling Using Local Wavelet SVM, IEEE Transactions on Instrumentation and 

Measurement, 59 (2010) 1485-1492. 

[13] A. Nebot, F. Cellier, M. Vallverdu, Mixed quantitative/qualitative modeling and 

simulation of the cardiovascular system, Computer Methods and Programs in Biomedicine, 

55 (1998) 127-155. 

[14] J. Cueva, R. Alquezar, A. Nebot, Experimental Comparison Of Fuzzy And Neural 

Network Techniques In Learning Models Of The Central Nervous System Control,  



29 
 

EUFIT'97, 5th European Congress on Intelligent Techniques and Soft ComputingAachen, 

1997, pp. 1014-1018. 

[15] A. Nebot, F. Mugica, F. Cellier, M. Vallverdu, Modeling and Simulation of the Central 

Nervous System Control with Generic Fuzzy Models, SIMULATION, 79 (2003) 648-669. 

[16] J. Acosta, Aprendizaje de Particiones Difusas para Razonamiento Inductivo,  

Departament dâ€™Enginyeria de Sistemes, AutomÃ tica i InformÃ tica Industrial, Universitat 

Politecnica de Catalunya, Barcelona, 2006. 

[17] P. GÃ³mez, A. Nebot, F. Mugica, Automatic Construction of Fuzzy Rules for Modelling 

and Prediction of the Central Nervous System,  Workshop de mineria de datos y aprendizaje 

automÃ¡tico en el marco de la 8th Iberoamerican Conference on Artificial IntelligenceSevilla, 

2002, pp. 23-32. 

[18] V. Vapnik, Statistical and learning theory, John Wiley & Sons, INC, New York, 1998. 

[19] F. Girosi, M. Jones, T. Poggio, Regularization theory and neural networks architectures, 

Neural computation, 7 (1995) 219-269. 

[20] M. Pontil, A. Verri, Properties of support vector machines, Neural Computation, 10 

(1998) 955-974. 

[21] C. Cortes, V. Vapnik, Support-vector networks, Machine learning, 20 (1995) 273-297. 

[22] F. Girosi, An equivalence between sparse approximation and support vector machines, 

Neural computation, 10 (1998) 1455-1480. 

[23] S.R. Gunn, Support vector machines for classification and regression, ISIS technical 

report, 14 (1998). 

[24] A.J. Smola, Regression estimation with support vector learning machines, Master's 

thesis, Technische Universitat Munchen, 1996. 



30 
 

[25] R. Matlab, version 7.14. 0.739 (R2012a), The MathWorks Inc Natick, MA. 

[26] C.-C. Chang, C.-J. Lin, LIBSVM: a library for support vector machines, ACM 

Transactions on Intelligent Systems and Technology (TIST), 2 (2011) 27. 

[27] C.-W. Hsu, C.-C. Chang, C.-J. Lin, A practical guide to support vector classification, 

Department of Computer Science, National Taiwan University, 2003. 

 


